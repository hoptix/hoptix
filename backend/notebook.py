# -*- coding: utf-8 -*-
"""Voice_Diarization_Prod_Aarav

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lgEZw16Wl4HxAVwQVnOHZ4ATH7Uab14-
"""

!pip install -U supabase Cython torch nemo_toolkit ffmpeg pydub pinecone assemblyai hydra-core pytorch_lightning huggingface_hub==0.23.5 librosa transformers pandas inflect webdataset sentencepiece youtokentome pyannote-audio editdistance jiwer lhotse datasets

AAI_API_KEY = "bc36d5b316fe4db491bd707d0a0cceee"
PINECONE_API_KEY="pcsk_4Lrovv_LpZoRcrgMrr7nNCXcny8bweBair4ia6cP2vKm52iWpGGabdMqXVGPaaWSX2ZnL1"
SUPABASE_URL="https://ohuumgemptxrqrdavslz.supabase.co"
SUPABASE_SERVICE_KEY="eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6Im9odXVtZ2VtcHR4cnFyZGF2c2x6Iiwicm9sZSI6InNlcnZpY2Vfcm9sZSIsImlhdCI6MTc1NTg5NzkzMSwiZXhwIjoyMDcxNDczOTMxfQ.KYfbCFuhh1RjOknbKiIsJqjzUibjbdUzOCmp_YyuZFk"

import assemblyai as aai
from pinecone import Pinecone, ServerlessSpec
import requests
import os
from pydub import AudioSegment
import mimetypes
import wave
from nemo.collections.asr.models import EncDecSpeakerLabelModel
import torch
import numpy as np
import uuid
from sklearn.metrics.pairwise import cosine_similarity
import glob
import uuid
import mimetypes
from typing import Dict, Tuple, List, Optional

# Obtain from your Pinecone dashboard.
pc = Pinecone(api_key=PINECONE_API_KEY)

# Obtain from your AssemblyAI dashboard.
aai.settings.api_key = AAI_API_KEY

from google.colab import drive
drive.mount('/content/drive')

# =========================
# CONFIG — EDIT THESE
# =========================
# Local folders
SAMPLES_DIR = "/content/drive/MyDrive/MISC/Hoptix/Cary Voice Samples" # Corrected 'hoptix' to 'Hoptix'
CLIPS_DIR   = "/content/drive/MyDrive/MISC/Hoptix/Clips_2025-10-04_0700"   # folder of tx_*.wav
CONVERTED_DIR = "./content/converted_audio"

# Matching knobs
THRESHOLD = 0.2         # raise for stricter; lower for more lenient
MIN_UTTERANCE_MS = 1000  # prefer >=5s segments for robust embeddings

# External secrets (set in env)
AAI_API_KEY = "bc36d5b316fe4db491bd707d0a0cceee"
PINECONE_API_KEY="pcsk_4Lrovv_LpZoRcrgMrr7nNCXcny8bweBair4ia6cP2vKm52iWpGGabdMqXVGPaaWSX2ZnL1"
SUPABASE_URL="https://ohuumgemptxrqrdavslz.supabase.co"
SUPABASE_SERVICE_KEY="eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6Im9odXVtZ2VtcHR4cnFyZGF2c2x6Iiwicm9sZSI6InNlcnZpY2Vfcm9sZSIsImlhdCI6MTc1NTg5NzkzMSwiZXhwIjoyMDcxNDczOTMxfQ.KYfbCFuhh1RjOknbKiIsJqjzUibjbdUzOCmp_YyuZFk"

# =========================
# UTILITIES / AUDIO HELPERS
# =========================
SUPPORTED_AUDIO_EXTS = {".wav", ".mp3", ".m4a", ".flac", ".aac", ".ogg", ".wma", ".mkv", ".mp4"}

def ensure_wav_mono(src_path: str, out_dir: str = CONVERTED_DIR) -> str:
    """
    Convert any local media (audio/video) to mono WAV and return the path.
    If the input is already a mono WAV, copy to out_dir and return that path.
    """
    os.makedirs(out_dir, exist_ok=True)
    base = os.path.splitext(os.path.basename(src_path))[0]
    wav_path = os.path.join(out_dir, f"{base}.wav")

    # If source is already the desired file and in out_dir, just return
    if os.path.abspath(src_path) == os.path.abspath(wav_path) and os.path.exists(wav_path):
        return wav_path

    audio = AudioSegment.from_file(src_path)
    if audio.channels > 1:
        audio = audio.set_channels(1)
    # Normalize sample rate to 16k (optional: depends on model) - keep original rate if you prefer
    audio = audio.set_frame_rate(16000)
    audio.export(wav_path, format="wav")
    return wav_path

def convert_folder_to_wavs(folder: str, out_dir: str = CONVERTED_DIR) -> List[str]:
    """
    Convert all supported audio files in `folder` to mono WAVs in out_dir.
    Returns a sorted list of output WAV paths.
    """
    os.makedirs(out_dir, exist_ok=True)
    all_files = sorted(
        p for p in glob.glob(os.path.join(folder, "*"))
        if os.path.splitext(p)[1].lower() in SUPPORTED_AUDIO_EXTS
    )

    wav_paths = []
    for p in all_files:
        try:
            wav = ensure_wav_mono(p, out_dir=out_dir)
            wav_paths.append(wav)
        except Exception as e:
            print(f"[WARN] Failed to convert {p}: {e}")

    return sorted(wav_paths)

def filename_to_label(path: str) -> str:
    """Turn 'Cary_Office01.wav' -> 'Cary Office01' (nicer label)."""
    stem = os.path.splitext(os.path.basename(path))[0]
    return stem.replace("_", " ").strip()

# =========================
# EMBEDDING HELPERS
# =========================
def l2norm_vec(x: np.ndarray) -> np.ndarray:
    x = np.asarray(x, dtype=float)
    n = np.linalg.norm(x)
    return x / (n + 1e-12)

def get_embedding_for_wav(wav_path: str, speaker_model) -> np.ndarray:
    """
    Compute a (192,) embedding using TitaNet for a WAV file.
    """
    emb = speaker_model.get_embedding(wav_path)
    if isinstance(emb, torch.Tensor):
        emb = emb.squeeze().cpu().numpy()
    else:
        emb = np.asarray(emb).squeeze()
    emb = emb.astype(float)
    if emb.shape != (192,):
        raise ValueError(f"Expected (192,) embedding, got {emb.shape} for {wav_path}")
    return l2norm_vec(emb)

def cosine_sim(a: np.ndarray, b: np.ndarray) -> float:
    return float(cosine_similarity(a.reshape(1, -1), b.reshape(1, -1))[0][0])

def avg_embed_top_n_utterances(
    audio: AudioSegment,
    utterances: List[dict],
    diarized_tag: str,
    speaker_model,
    top_n: int = 3,
) -> Optional[np.ndarray]:
    """
    Average embeddings from the top_n longest utterances for the diarized_tag.
    Returns L2-normalized averaged vector or None.
    """
    utts = [u for u in utterances if u["speaker"] == diarized_tag]
    if not utts:
        return None
    top = sorted(utts, key=lambda u: (u["end"] - u["start"]), reverse=True)[:top_n]
    embs = []
    for u in top:
        start_ms, end_ms = int(u["start"]), int(u["end"])
        seg = audio[start_ms:end_ms]
        tmp = os.path.join(tempfile.gettempdir(), f"tmp_avg_{uuid.uuid4().hex[:8]}.wav")
        seg.export(tmp, format="wav")
        try:
            e = get_embedding_for_wav(tmp, speaker_model)
            embs.append(l2norm_vec(e))
        finally:
            try: os.remove(tmp)
            except: pass
    if not embs:
        return None
    avg = np.vstack(embs).mean(axis=0)
    return l2norm_vec(avg)

def concat_and_embed_until_length(
    audio: AudioSegment,
    utterances: List[dict],
    diarized_tag: str,
    speaker_model,
    target_ms: int = 8000,
    max_utts: int = 6,
) -> Optional[np.ndarray]:
    """
    Concatenate utterances in chronological order for diarized_tag until total >= target_ms
    (or max_utts reached), embed the concatenation, and return L2-normalized embedding.
    """
    utts = sorted([u for u in utterances if u["speaker"] == diarized_tag], key=lambda u: u["start"])
    if not utts:
        return None
    pieces = []
    total = 0
    count = 0
    for u in utts:
        start_ms, end_ms = int(u["start"]), int(u["end"])
        seg = audio[start_ms:end_ms]
        pieces.append(seg)
        total += len(seg)
        count += 1
        if total >= target_ms or count >= max_utts:
            break
    if not pieces:
        return None
    concat = pieces[0]
    for p in pieces[1:]:
        concat += p
    tmp = os.path.join(tempfile.gettempdir(), f"tmp_cat_{uuid.uuid4().hex[:8]}.wav")
    concat.export(tmp, format="wav")
    try:
        e = get_embedding_for_wav(tmp, speaker_model)
        return l2norm_vec(e)
    finally:
        try: os.remove(tmp)
        except: pass

def pick_suitable_snippet(utterances: List[dict], speaker_tag: str, min_len_ms: int):
    """
    Choose the longest utterance for a diarized speaker, preferably >= min_len_ms.
    Returns the utterance dict or None.
    """
    candidates = [u for u in utterances if u["speaker"] == speaker_tag]
    if not candidates:
        return None
    long_enough = [u for u in candidates if (u["end"] - u["start"]) >= min_len_ms]
    if long_enough:
        return max(long_enough, key=lambda u: u["end"] - u["start"])
    return max(candidates, key=lambda u: u["end"] - u["start"])

# =========================
# ASSEMBLYAI (transcription)
# =========================
def transcribe_with_speaker_labels(local_media_path: str) -> dict:
    """
    Transcribe a LOCAL file with AssemblyAI and return json_response.
    (AssemblyAI's SDK accepts local paths directly.)
    """
    if AAI_API_KEY is None or AAI_API_KEY.startswith("<"):
        raise RuntimeError("Set AAI_API_KEY environment variable before calling transcribe_with_speaker_labels()")
    aai.settings.api_key = AAI_API_KEY
    config = aai.TranscriptionConfig(speaker_labels=True)
    transcriber = aai.Transcriber(config=config)
    transcript = transcriber.transcribe(local_media_path)
    return transcript.json_response

# =========================
# MATCHING / DIARIZATION-TO-SAMPLE MAPPING
# =========================
def identify_speakers_in_transcript(
    transcript_json: dict,
    wav_path: str,
    speaker_model,
    local_embeddings: Dict[str, np.ndarray],
    threshold: float = THRESHOLD,
    min_utterance_ms: int = MIN_UTTERANCE_MS,
) -> Tuple[List[dict], Dict[str, bool]]:
    """
    For each diarized speaker cluster, create a robust diarized embedding by
    averaging or concatenating multiple utterances, map to known samples, and
    relabel utterances. Returns relabeled utterances and presence map.
    """
    utterances = transcript_json.get("utterances", [])
    if not utterances:
        raise ValueError("No 'utterances' found in transcript. Make sure speaker_labels=True.")

    audio = AudioSegment.from_wav(wav_path)

    diarized_to_label: Dict[str, str] = {}

    # iterate diarized tags (e.g. 'A', 'B')
    for diarized_speaker in sorted(set(u["speaker"] for u in utterances)):
        # Try: 1) average top-N utterances (more stable)
        emb = avg_embed_top_n_utterances(audio, utterances, diarized_speaker, speaker_model, top_n=3)

        # If not enough long utts or avg failed, fallback to concatenation until length
        if emb is None:
            emb = concat_and_embed_until_length(audio, utterances, diarized_speaker, speaker_model, target_ms=8000, max_utts=8)

        # If still None, pick the single longest utterance (last resort)
        if emb is None:
            chosen = pick_suitable_snippet(utterances, diarized_speaker, min_utterance_ms)
            if chosen:
                tmp = os.path.join(tempfile.gettempdir(), f"temp_{uuid.uuid4().hex[:8]}.wav")
                seg = audio[int(chosen["start"]):int(chosen["end"])]
                seg.export(tmp, format="wav")
                try:
                    e = get_embedding_for_wav(tmp, speaker_model)
                    emb = l2norm_vec(e)
                finally:
                    try: os.remove(tmp)
                    except: pass

        if emb is None:
            print(f"[Map] diarized '{diarized_speaker}' -> 'No match' (no usable audio)")
            diarized_to_label[diarized_speaker] = "No match"
            continue

        # emb already normalized by helpers
        best_label = "No match"
        best_score = 0.0
        for label, ref_emb in local_embeddings.items():
            # ensure ref_emb is numpy array and normalized
            ref = l2norm_vec(np.asarray(ref_emb, dtype=float))
            score = float(cosine_similarity(emb.reshape(1, -1), ref.reshape(1, -1))[0][0])
            if score > best_score:
                best_label, best_score = label, score

        # apply threshold
        if best_score < threshold:
            print(f"[Map] diarized '{diarized_speaker}' -> 'No match' (best={best_label}, score={best_score:.3f})")
            diarized_to_label[diarized_speaker] = "No match"
        else:
            print(f"[Map] diarized '{diarized_speaker}' -> '{best_label}' (score={best_score:.3f})")
            diarized_to_label[diarized_speaker] = best_label

    # Relabel every utterance’s 'speaker' to the matched sample name (or 'No match')
    relabeled = []
    for u in utterances:
        src = dict(u)  # shallow copy
        src["speaker_original"] = src["speaker"]
        src["speaker"] = diarized_to_label.get(src["speaker_original"], "No match")
        relabeled.append(src)

    # Presence: a sample is present if it appears in any relabeled utterance
    presence = {label: False for label in local_embeddings.keys()}
    for u in relabeled:
        if u["speaker"] in presence:
            presence[u["speaker"]] = True

    return relabeled, presence

def find_closest_speaker(
    utterance_embedding: np.ndarray,
    local_embeddings: Dict[str, np.ndarray],
    threshold: float = THRESHOLD,
) -> Tuple[str, float]:
    """
    Compare an utterance embedding to all known sample embeddings and
    return (best_label, score) if above threshold; otherwise ("No match", 0).
    """
    best_label = "No match"
    best_score = 0.0
    for label, emb in local_embeddings.items():
        score = cosine_sim(utterance_embedding, l2norm_vec(np.asarray(emb, dtype=float)))
        if score > best_score:
            best_label, best_score = label, score

    if best_score < threshold:
        return "No match", 0.0
    return best_label, best_score

# =========================
# HIGH-LEVEL RUNNER
# =========================
def run_pipeline_on_media_best_match(
    local_media_path: str,
    speaker_model,
    local_embeddings: Dict[str, np.ndarray],
    threshold: float = THRESHOLD,
    min_utterance_ms: int = MIN_UTTERANCE_MS
):
    """Run pipeline and pick the speaker that best matches local embeddings, ignoring dominance."""
    wav_path = ensure_wav_mono(local_media_path)
    transcript_json = transcribe_with_speaker_labels(wav_path)
    relabeled_utterances, presence = identify_speakers_in_transcript(
        transcript_json=transcript_json,
        wav_path=wav_path,
        speaker_model=speaker_model,
        local_embeddings=local_embeddings,
        threshold=threshold,
        min_utterance_ms=min_utterance_ms,
    )

    raw_utterances = transcript_json.get("utterances", [])
    audio = AudioSegment.from_wav(wav_path)

    best_score = 0.0
    best_label = "No match"

    # Iterate through all relabeled speakers and compute similarity
    unique_speakers = set(u["speaker"] for u in relabeled_utterances)
    for sp in unique_speakers:
        emb_parts = []
        mapped_tags = sorted(set(u["speaker_original"] for u in relabeled_utterances if u["speaker"] == sp))
        for tag in mapped_tags:
            e = avg_embed_top_n_utterances(audio, raw_utterances, tag, speaker_model, top_n=3)
            if e is None:
                e = concat_and_embed_until_length(audio, raw_utterances, tag, speaker_model, target_ms=10000, max_utts=8)
            if e is not None:
                emb_parts.append(e)

        if emb_parts:
            sp_emb = l2norm_vec(np.vstack(emb_parts).mean(axis=0))
            ref = local_embeddings.get(sp)
            if ref is not None:
                score = float(cosine_similarity(sp_emb.reshape(1, -1), l2norm_vec(np.asarray(ref)).reshape(1, -1))[0][0])
                if score > best_score:
                    best_score = score
                    best_label = sp

    return best_label, best_score, relabeled_utterances, presence

# =========================
# BOOTSTRAP: load model + sample embeddings
# =========================
def build_local_embeddings(samples_dir: str, speaker_model) -> Dict[str, np.ndarray]:
    sample_paths = sorted(glob.glob(os.path.join(samples_dir, "*.wav")))
    if not sample_paths:
        raise SystemExit(f"No .wav files found in {samples_dir}")
    local_embeddings: Dict[str, np.ndarray] = {}
    for p in sample_paths:
        lbl = filename_to_label(p)
        wav = ensure_wav_mono(p)
        emb = get_embedding_for_wav(wav, speaker_model)
        local_embeddings[lbl] = emb
        print("  - embedded:", lbl)
    return local_embeddings

import supabase

print(SUPABASE_URL, SUPABASE_SERVICE_KEY)

client = supabase.create_client(SUPABASE_URL, SUPABASE_SERVICE_KEY)

def get_worker_name_to_id():
    """
    Fetch workers from supabase and return a mapping {legal_name: id}.
    Uses .execute() to run the request builder and reads resp.data safely.
    """
    try:
        # call .execute() to actually run the request
        resp = client.table("workers").select("id, legal_name").execute()
    except Exception as e:
        print("Supabase request failed:", e)
        return {}

    # older/newer supabase clients sometimes return dict-like or object
    data = None
    if hasattr(resp, "data"):
        data = resp.data
    elif isinstance(resp, dict) and "data" in resp:
        data = resp["data"]
    else:
        # Try common fallback shapes
        try:
            data = resp.get("data")  # if resp is dict-like
        except Exception:
            data = None

    if data is None:
        print("Warning: received unexpected response shape from supabase:", type(resp), resp)
        return {}

    worker_map = {}
    for worker in data:
        # be defensive: keys might be 'legal_name' or 'name'
        name = worker.get("legal_name") or worker.get("name") or worker.get("display_name")
        wid = worker.get("id")
        if name and wid:
            worker_map[name] = wid
    return worker_map

WORKER_NAME_TO_ID = get_worker_name_to_id()
print("WORKER_NAME_TO_ID:", WORKER_NAME_TO_ID)

import supabase
import time # Import the time module

print(SUPABASE_URL, SUPABASE_SERVICE_KEY)

client = supabase.create_client(SUPABASE_URL, SUPABASE_SERVICE_KEY)

def get_worker_name_to_id():
    """
    Fetch workers from supabase and return a mapping {legal_name: id}.
    Uses .execute() to run the request builder and reads resp.data safely.
    """
    try:
        # call .execute() to actually run the request
        resp = client.table("workers").select("id, legal_name").execute()
    except Exception as e:
        print("Supabase request failed:", e)
        return {}

    # older/newer supabase clients sometimes return dict-like or object
    data = None
    if hasattr(resp, "data"):
        data = resp.data
    elif isinstance(resp, dict) and "data" in resp:
        data = resp["data"]
    else:
        # Try common fallback shapes
        try:
            data = resp.get("data")  # if resp is dict-like
        except Exception:
            data = None

    if data is None:
        print("Warning: received unexpected response shape from supabase:", type(resp), resp)
        return {}

    worker_map = {}
    for worker in data:
        # be defensive: keys might be 'legal_name' or 'name'
        name = worker.get("legal_name") or worker.get("name") or worker.get("display_name")
        wid = worker.get("id")
        if name and wid:
            worker_map[name] = wid
    return worker_map

WORKER_NAME_TO_ID = get_worker_name_to_id()
print("WORKER_NAME_TO_ID:", WORKER_NAME_TO_ID)

def check_complete_transactions_or_labeled(transaction_id, retries=3, delay=5):
    """
    Checks if a transaction is complete or already labeled in the graded_rows_filtered table with retries.
    Returns True if the transaction should be skipped, False otherwise.
    """
    for i in range(retries):
        try:
            transaction = client.table("graded_rows_filtered").select("*").eq("transaction_id", transaction_id).execute()

            # Check if any data was returned
            if not transaction.data:
                print(f"Transaction {transaction_id} not found in graded_rows_filtered table. Skipping.")
                return True

            # Access the first element only if data exists
            if not transaction.data[0].get('complete_order'):
                print(f"Order for transaction {transaction_id} not complete. Skipping.")
                return True

            if transaction.data[0].get('worker_id') is not None:
                print(f"Worker for transaction {transaction_id} already labeled. Skipping.")
                return True

            return False  # If successful, exit the retry loop

        except Exception as e:
            if i < retries - 1:
                print(f"Attempt {i+1} failed for transaction {transaction_id}: {e}. Retrying in {delay} seconds...")
                time.sleep(delay)
            else:
                print(f"Failed to check transaction {transaction_id} after {retries} attempts: {e}")
                return True # Skip if all retries fail

    return True # Should not reach here, but as a fallback

check_complete_transactions_or_labeled("56f2f41f-7676-40c5-af87-6a488e976256")

import re
import glob
import os
import tempfile
from concurrent.futures import ThreadPoolExecutor, as_completed

# =========================
# MAIN: iterate clips and update Supabase transactions
# =========================

def process_clip(clip, speaker_model, local_embeddings, LABEL_TO_WORKER_ID):
    updated, no_match, failures = 0, 0, 0
    base = os.path.basename(clip)

    # Updated regex to match both .wav and .mp3 files
    m = re.match(r"tx_([0-9a-fA-F-]{36})\.(wav|mp3)$", base, re.IGNORECASE)
    if not m:
        print("Skip (bad name):", base)
        return updated, no_match, failures

    tx_id = m.group(1)
    print("\n=== TX", tx_id, "===")

    if check_complete_transactions_or_labeled(tx_id):
        return updated, no_match, failures

    try:
        best_label, score, relabeled, presence = run_pipeline_on_media_best_match(
            local_media_path=clip,
            speaker_model=speaker_model,
            local_embeddings=local_embeddings,
            threshold=THRESHOLD,
            min_utterance_ms=MIN_UTTERANCE_MS,
        )
        print("Best match speaker:", best_label, "score:", score)

        worker_id = LABEL_TO_WORKER_ID.get(best_label)
        if not worker_id or best_label == "No match":
            print("No match over threshold; leaving worker_id NULL.")
            no_match += 1
            return updated, no_match, failures

        # Update supabase (if client available)
        if SUPABASE_URL.startswith("<") or SUPABASE_SERVICE_KEY.startswith("<"):
            print(f"[DRY] Would update tx {tx_id} -> worker_id={worker_id}, confidence={score:.3f}")
        else:
            resp = client.table("transactions").update({
                "worker_id": worker_id,
                "worker_assignment_source": "voice",
                "worker_confidence": float(score),
            }).eq("id", tx_id).execute()
            print("Supabase update response:", resp)

        updated += 1
        print(f"UPDATED worker_id={worker_id}, confidence={score:.3f}")

    except Exception as e:
        failures += 1
        print("FAILED:", e)

    return updated, no_match, failures


def main():
    # Load speaker model (TitaNet)
    print("Loading TitaNet…")
    speaker_model = EncDecSpeakerLabelModel.from_pretrained("nvidia/speakerverification_en_titanet_large")

    print("Indexing reference voices:", SAMPLES_DIR)
    local_embeddings = build_local_embeddings(SAMPLES_DIR, speaker_model)

    # Map filename-derived labels to worker ids
    LABEL_TO_WORKER_ID: Dict[str, str] = {}
    for lbl in local_embeddings.keys():
        if lbl in WORKER_NAME_TO_ID:
            LABEL_TO_WORKER_ID[lbl] = WORKER_NAME_TO_ID[lbl]
            continue
        last = lbl.split()[-1].lower()
        for name, wid in WORKER_NAME_TO_ID.items():
            if last == name.split()[-1].lower():
                LABEL_TO_WORKER_ID[lbl] = wid
                break

    print("Label→worker_id:")
    for k, v in LABEL_TO_WORKER_ID.items():
        print(" ", k, "→", v)

    # Updated to find both .wav and .mp3 files
    wav_clips = glob.glob(os.path.join(CLIPS_DIR, "tx_*.wav"))
    mp3_clips = glob.glob(os.path.join(CLIPS_DIR, "tx_*.mp3"))
    clip_paths = sorted(wav_clips + mp3_clips)

    print(f"Found {len(clip_paths)} clips in {CLIPS_DIR} ({len(wav_clips)} WAV, {len(mp3_clips)} MP3)")

    updated_total, no_match_total, failures_total = 0, 0, 0

    # Parallelize with 5 threads
    with ThreadPoolExecutor(max_workers=5) as executor:
        futures = {executor.submit(process_clip, clip, speaker_model, local_embeddings, LABEL_TO_WORKER_ID): clip for clip in clip_paths}
        for future in as_completed(futures):
            updated, no_match, failures = future.result()
            updated_total += updated
            no_match_total += no_match
            failures_total += failures

    print("\n--- Summary ---")
    print("Updated rows:", updated_total)
    print("No match:", no_match_total)
    print("Failures:", failures_total)


if __name__ == "__main__":
    main()